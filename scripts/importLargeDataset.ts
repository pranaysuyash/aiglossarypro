import fs from 'node:fs';
import { importProcessedData } from './server/pythonProcessor';

async function importLargeDataset() {
  try {
    const largeFile = 'temp/processed_1750505973480.json';

    console.log('ğŸ” Checking for large processed dataset...');

    if (!fs.existsSync(largeFile)) {
      console.error(`âŒ Large dataset file not found: ${largeFile}`);
      console.log('ğŸ’¡ To generate the dataset, run the Excel processor on data/aiml.xlsx');
      console.log('   This will process the 286MB Excel file with 10k+ terms');
      return;
    }

    console.log('ğŸ“Š Loading large processed dataset...');
    const data = JSON.parse(fs.readFileSync(largeFile, 'utf8'));

    console.log(`ğŸ“ˆ Dataset summary:`);
    console.log(`   ğŸ“‚ Categories: ${data.categories?.length || 0}`);
    console.log(`   ğŸ“‹ Subcategories: ${data.subcategories?.length || 0}`);
    console.log(`   ğŸ“Š Terms: ${data.terms?.length || 0}`);

    if (data.terms?.length < 5000) {
      console.warn('âš ï¸  Dataset seems smaller than expected. Proceeding anyway...');
    }

    console.log('ğŸ’¾ Importing into database (this may take a few minutes)...');
    const startTime = Date.now();

    const result = await importProcessedData(data);

    const endTime = Date.now();
    const duration = (endTime - startTime) / 1000;

    if (result.success) {
      console.log('âœ… Large dataset import successful!');
      console.log(`   â±ï¸  Import time: ${duration.toFixed(2)} seconds`);
      console.log(`   ğŸ“‚ Categories imported: ${result.imported.categories}`);
      console.log(`   ğŸ“‹ Subcategories imported: ${result.imported.subcategories}`);
      console.log(`   ğŸ“Š Terms imported: ${result.imported.terms}`);
      console.log('');
      console.log('ğŸ‰ Your AI Glossary now has the complete 10k+ term dataset!');
      console.log('ğŸ’¡ Large processed files remain in gitignore to keep repo size manageable');
    } else {
      console.error('âŒ Import failed:', result.error);
    }
  } catch (error) {
    console.error('âŒ Import error:', error);
    console.log('ğŸ’¡ If you encounter memory issues, try processing in chunks');
  }
}

// Also provide instructions for regenerating the dataset
console.log('ğŸš€ AI Glossary Pro - Large Dataset Importer');
console.log('');
console.log('ğŸ“‹ This script imports the complete 10k+ term dataset from processed JSON');
console.log('ğŸ”’ Large files remain in gitignore to keep repository size manageable');
console.log('');

importLargeDataset();
