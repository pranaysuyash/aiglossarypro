import fs from 'fs';
import path from 'path';
import { runPythonExcelProcessor, importProcessedData } from './pythonProcessor';
import { cacheManager } from './cacheManager';
import { aiChangeDetector } from './aiChangeDetector';
import * as XLSX from 'xlsx';

/**
 * Smart Excel loader that chooses the appropriate processing method based on file size
 * - Uses chunked Python processor for large files (>50MB) with memory optimization
 * - Uses JavaScript parser for smaller files
 * - Provides progress tracking and resumable processing
 */

const MAX_JS_FILE_SIZE = 50 * 1024 * 1024; // 50MB threshold
const DEFAULT_CHUNK_SIZE = 500; // Process 500 rows per chunk

interface ProcessingOptions {
  chunkSize?: number;
  enableProgress?: boolean;
  resumeProcessing?: boolean;
}

interface FileAnalysis {
  fileSizeMB: number;
  columnCount: number;
  estimatedRowCount: number;
  isComplexStructure: boolean;
  recommendedProcessor: 'simple' | 'advanced' | 'chunked' | 'streaming';
  processingStrategy: string;
}

/**
 * Analyze Excel file structure to determine optimal processing strategy
 */
async function analyzeExcelFile(filePath: string): Promise<FileAnalysis> {
  console.log('\nüîç ANALYZING EXCEL FILE STRUCTURE');
  console.log('=====================================');
  
  try {
    const stats = fs.statSync(filePath);
    const fileSizeMB = stats.size / (1024 * 1024);
    
    console.log(`üìä File size: ${fileSizeMB.toFixed(2)} MB`);
    
    // For very large files (>200MB), skip detailed analysis and route to streaming
    if (fileSizeMB > 200) {
      console.log('üåä Large file detected (>200MB) - routing to streaming processor');
      return {
        fileSizeMB,
        columnCount: 295, // Known from 42-section structure
        estimatedRowCount: Math.floor(fileSizeMB * 35),
        isComplexStructure: true, // Assume complex for large files
        recommendedProcessor: 'streaming',
        processingStrategy: 'Very large file ‚Üí Streaming processor with memory optimization'
      };
    }
    
    // For smaller files, try to analyze structure
    console.log('üìÇ Reading Excel file for analysis...');
    const buffer = fs.readFileSync(filePath);
    console.log(`üìä Buffer size: ${buffer.length} bytes`);
    
    let workbook;
    let columnCount = 1;
    let headers: string[] = [];
    
    try {
      // Try to read just the header row for analysis
      workbook = XLSX.read(buffer, {
        type: 'buffer',
        sheetRows: 1,
        cellText: false
      });
      
      const sheetName = workbook.SheetNames[0];
      const worksheet = workbook.Sheets[sheetName];
      
      if (worksheet && worksheet['!ref']) {
        const range = XLSX.utils.decode_range(worksheet['!ref']);
        columnCount = range.e.c + 1;
        
        // Extract headers
        for (let col = 0; col < columnCount; col++) {
          const cellAddress = XLSX.utils.encode_cell({ r: 0, c: col });
          const cell = worksheet[cellAddress];
          if (cell && cell.v) {
            headers.push(String(cell.v));
          }
        }
      }
    } catch (error) {
      console.warn('‚ö†Ô∏è  Header analysis failed, using file size heuristics');
      // Fallback to size-based detection
      columnCount = fileSizeMB > 50 ? 295 : 10; // Estimate based on size
    }
    
    console.log(`üìã Column count: ${columnCount}`);
    console.log(`üìä Estimated row count: ${Math.floor(fileSizeMB * 35)}`);
    
    // Detect complex structure
    const complexStructureKeywords = [
      'Introduction ‚Äì',
      'Prerequisites ‚Äì',
      'Theoretical Concepts ‚Äì',
      'Implementation ‚Äì',
      'Tags and Keywords ‚Äì'
    ];
    
    const hasComplexStructure = columnCount > 100 || 
      complexStructureKeywords.some(keyword => 
        headers.some(header => header.includes(keyword))
      );
    
    console.log(`üèóÔ∏è  Complex 42-section structure detected: ${hasComplexStructure ? 'YES' : 'NO'}`);
    
    // Determine processing strategy
    let recommendedProcessor: FileAnalysis['recommendedProcessor'];
    let processingStrategy: string;
    
    if (hasComplexStructure) {
      if (fileSizeMB > 100) {
        recommendedProcessor = 'streaming';
        processingStrategy = 'Large complex file ‚Üí Streaming processor with 42-section parsing';
      } else {
        recommendedProcessor = 'advanced';
        processingStrategy = 'Complex structure ‚Üí Advanced AI-powered parser';
      }
    } else {
      if (fileSizeMB > 50) {
        recommendedProcessor = 'chunked';
        processingStrategy = 'Large simple file ‚Üí Chunked processor';
      } else {
        recommendedProcessor = 'simple';
        processingStrategy = 'Small simple file ‚Üí JavaScript parser';
      }
    }
    
    console.log(`üéØ Recommended processor: ${recommendedProcessor.toUpperCase()}`);
    console.log(`üìã Strategy: ${processingStrategy}`);
    
    return {
      fileSizeMB,
      columnCount,
      estimatedRowCount: Math.floor(fileSizeMB * 35),
      isComplexStructure: hasComplexStructure,
      recommendedProcessor,
      processingStrategy
    };
    
  } catch (error) {
    console.error('‚ùå Error analyzing Excel file:', error);
    // Fallback analysis based on file size
    const stats = fs.statSync(filePath);
    const fileSizeMB = stats.size / (1024 * 1024);
    
    return {
      fileSizeMB,
      columnCount: fileSizeMB > 100 ? 295 : 10,
      estimatedRowCount: Math.floor(fileSizeMB * 35),
      isComplexStructure: fileSizeMB > 100,
      recommendedProcessor: fileSizeMB > 200 ? 'streaming' : 'advanced',
      processingStrategy: 'Fallback analysis ‚Üí Size-based routing'
    };
  }
}

/**
 * Enhanced smart Excel loader with comprehensive logging and proper routing
 */
export async function smartLoadExcelData(filePath: string, options: ProcessingOptions = {}, forceReprocess: boolean = false): Promise<void> {
  const startTime = Date.now();
  console.log('\nüöÄ STARTING SMART EXCEL PROCESSING');
  console.log('===================================');
  console.log(`üìÇ File: ${path.basename(filePath)}`);
  console.log(`‚ö° Force reprocess: ${forceReprocess ? 'YES' : 'NO'}`);
  console.log(`üîß Options:`, options);
  
  try {
    // Step 1: File Validation
    console.log('\nüìã STEP 1: FILE VALIDATION');
    console.log('---------------------------');
    
    if (!fs.existsSync(filePath)) {
      console.error(`‚ùå CRITICAL ERROR: File not found at ${filePath}`);
      throw new Error(`File not found: ${filePath}`);
    }
    console.log('‚úÖ File exists');
    
    // Step 2: File Analysis
    console.log('\nüìä STEP 2: FILE ANALYSIS');
    console.log('-------------------------');
    
    const analysis = await analyzeExcelFile(filePath);
    console.log('‚úÖ File analysis completed');
    
    // Step 3: Cache Check (unless forced)
    if (!forceReprocess) {
      console.log('\nüíæ STEP 3: CACHE VALIDATION');
      console.log('----------------------------');
      
      const isCacheValid = await cacheManager.isCacheValid(filePath);
      console.log(`üîç Cache valid: ${isCacheValid ? 'YES' : 'NO'}`);
      
      if (isCacheValid) {
        console.log('‚ö° Loading from cache...');
        const cachedData = await cacheManager.loadFromCache(filePath);
        if (cachedData) {
          console.log('‚úÖ Cache data loaded, importing to database...');
          
          const success = await importCachedData(cachedData);
          if (success) {
            const totalTime = (Date.now() - startTime) / 1000;
            console.log('\nüéâ PROCESSING COMPLETED SUCCESSFULLY (CACHED)');
            console.log('==============================================');
            console.log(`‚è±Ô∏è  Total time: ${totalTime.toFixed(2)} seconds`);
            return;
          } else {
            console.warn('‚ö†Ô∏è  Cache import failed, proceeding with fresh processing');
          }
        }
      }
    } else {
      console.log('\nüíæ STEP 3: CACHE BYPASS (FORCED REPROCESS)');
      console.log('-------------------------------------------');
      console.log('üîÑ Skipping cache check due to force reprocess flag');
    }
    
    // Step 4: Processing Route Selection
    console.log('\nüéØ STEP 4: PROCESSING ROUTE SELECTION');
    console.log('--------------------------------------');
    console.log(`üìã Selected processor: ${analysis.recommendedProcessor.toUpperCase()}`);
    console.log(`üõ†Ô∏è  Strategy: ${analysis.processingStrategy}`);
    
    // Step 5: Execute Processing
    console.log('\n‚öôÔ∏è  STEP 5: PROCESSING EXECUTION');
    console.log('--------------------------------');
    
    let success = false;
    switch (analysis.recommendedProcessor) {
      case 'advanced':
        success = await processWithAdvancedParser(filePath, analysis, options);
        break;
        
      case 'streaming': 
        success = await processWithStreamingParser(filePath, analysis, options);
        break;
        
      case 'chunked':
        success = await processWithChunkedParser(filePath, analysis, options);
        break;
        
      case 'simple':
        success = await processWithSimpleParser(filePath, analysis, options);
        break;
        
      default:
        throw new Error(`Unknown processor type: ${analysis.recommendedProcessor}`);
    }
    
    // Step 6: Final Validation
    console.log('\n‚úÖ STEP 6: FINAL VALIDATION');
    console.log('----------------------------');
    
    if (success) {
      const totalTime = (Date.now() - startTime) / 1000;
      console.log('\nüéâ PROCESSING COMPLETED SUCCESSFULLY');
      console.log('====================================');
      console.log(`üìä File: ${path.basename(filePath)}`);
      console.log(`üìã Processor: ${analysis.recommendedProcessor.toUpperCase()}`);
      console.log(`‚è±Ô∏è  Total time: ${totalTime.toFixed(2)} seconds`);
      console.log(`üìà Strategy: ${analysis.processingStrategy}`);
      
      // Validate database state
      await validateDatabaseState();
    } else {
      throw new Error('Processing completed but failed validation checks');
    }
    
  } catch (error) {
    const totalTime = (Date.now() - startTime) / 1000;
    console.error('\nüí• PROCESSING FAILED');
    console.error('====================');
    console.error(`‚ùå Error: ${error instanceof Error ? error.message : 'Unknown error'}`);
    console.error(`‚è±Ô∏è  Failed after: ${totalTime.toFixed(2)} seconds`);
    console.error(`üìÇ File: ${path.basename(filePath)}`);
    
    if (error instanceof Error && error.stack) {
      console.error('üìã Stack trace:');
      console.error(error.stack);
    }
    
    throw error;
  }
}

/**
 * Import cached data with proper error handling and logging
 */
async function importCachedData(cachedData: any): Promise<boolean> {
  try {
    console.log('üîÑ Importing cached data to database...');
    
    const { batchedImportProcessedData } = await import('./batchedImporter');
    
    // Save cached data to temp file for batched import
    const tempDir = path.join(process.cwd(), 'temp');
    if (!fs.existsSync(tempDir)) {
      fs.mkdirSync(tempDir, { recursive: true });
    }
    const tempCacheFile = path.join(tempDir, `cached_${Date.now()}.json`);
    fs.writeFileSync(tempCacheFile, JSON.stringify(cachedData));
    
    const importResult = await batchedImportProcessedData(tempCacheFile, {
      batchSize: 500,
      skipExisting: true
    });
    
    // Clean up temp file
    fs.unlinkSync(tempCacheFile);
    
    if (importResult.success) {
      console.log(`‚úÖ Cache data imported successfully:`);
      console.log(`   üìÇ ${importResult.imported.categories} categories`);
      console.log(`   üìã ${importResult.imported.subcategories} subcategories`);
      console.log(`   üìä ${importResult.imported.terms} terms`);
      return true;
    } else {
      console.error('‚ùå Cache import failed:', importResult.errors);
      return false;
    }
    
  } catch (error) {
    console.error('‚ùå Error importing cached data:', error);
    return false;
  }
}

/**
 * Process with Advanced Parser (for 42-section complex files)
 */
async function processWithAdvancedParser(filePath: string, analysis: FileAnalysis, options: ProcessingOptions): Promise<boolean> {
  console.log('üß† Starting ADVANCED PARSER processing...');
  console.log('üìã Using AI-powered 42-section extraction');
  
  try {
    const { AdvancedExcelParser, importComplexTerms } = await import('./advancedExcelParser');
    
    const parser = new AdvancedExcelParser();
    const buffer = fs.readFileSync(filePath);
    
    console.log('üîÑ Parsing complex Excel structure...');
    const parsedTerms = await parser.parseComplexExcel(buffer);
    
    console.log(`‚úÖ Parsed ${parsedTerms.length} complex terms`);
    console.log('üîÑ Importing to enhanced database schema...');
    
    await importComplexTerms(parsedTerms);
    
    console.log('‚úÖ Advanced parser processing completed successfully');
    return true;
    
  } catch (error) {
    console.error('‚ùå Advanced parser failed:', error);
    return false;
  }
}

/**
 * Process with Streaming Parser (for very large complex files)
 */
async function processWithStreamingParser(filePath: string, analysis: FileAnalysis, options: ProcessingOptions): Promise<boolean> {
  console.log('üåä Starting STREAMING PARSER processing...');
  console.log('üìã Using memory-efficient streaming with 42-section support');
  
  try {
    const { StreamingExcelProcessor } = await import('../streaming_excel_processor');
    
    const processor = new StreamingExcelProcessor({
      batchSize: options.chunkSize || 25,
      maxMemoryMB: 1024,
      progressCallback: options.enableProgress ? (processed, total) => {
        if (processed % 100 === 0) {
          const percentage = ((processed / total) * 100).toFixed(1);
          console.log(`üìà Streaming progress: ${percentage}% (${processed}/${total})`);
        }
      } : undefined
    });
    
    await processor.processFile(filePath);
    
    console.log('‚úÖ Streaming parser processing completed successfully');
    return true;
    
  } catch (error) {
    console.error('‚ùå Streaming parser failed:', error);
    return false;
  }
}

/**
 * Process with Chunked Parser (for large simple files)
 */
async function processWithChunkedParser(filePath: string, analysis: FileAnalysis, options: ProcessingOptions): Promise<boolean> {
  console.log('üì¶ Starting CHUNKED PARSER processing...');
  console.log('üìã Using chunk-based processing for large files');
  
  try {
    const { ChunkedExcelProcessor } = await import('../chunked_excel_processor');
    
    const processor = new ChunkedExcelProcessor({
      chunkSize: options.chunkSize || 50,
      batchSize: 10
    });
    
    await processor.processFile(filePath);
    
    console.log('‚úÖ Chunked parser processing completed successfully');
    return true;
    
  } catch (error) {
    console.error('‚ùå Chunked parser failed:', error);
    return false;
  }
}

/**
 * Process with Simple Parser (for small simple files)
 */
async function processWithSimpleParser(filePath: string, analysis: FileAnalysis, options: ProcessingOptions): Promise<boolean> {
  console.log('üìÑ Starting SIMPLE PARSER processing...');
  console.log('üìã Using JavaScript-based parser for small files');
  
  try {
    const { parseExcelFile, importToDatabase } = await import('./excelParser');
    
    const buffer = fs.readFileSync(filePath);
    console.log('üîÑ Parsing Excel file...');
    const parsedData = await parseExcelFile(buffer);
    
    console.log(`‚úÖ Parsed ${parsedData.terms.length} terms`);
    console.log('üîÑ Importing to database...');
    
    const result = await importToDatabase(parsedData);
    
    console.log(`‚úÖ Imported ${result.termsImported} terms and ${result.categoriesImported} categories`);
    return true;
    
  } catch (error) {
    console.error('‚ùå Simple parser failed:', error);
    return false;
  }
}

/**
 * Validate database state after processing
 */
async function validateDatabaseState(): Promise<void> {
  console.log('üîç Validating database state...');
  
  try {
    // TODO: Add actual database validation queries
    // For now, just log that validation is needed
    console.log('‚ö†Ô∏è  Database validation not yet implemented');
    console.log('‚úÖ Skipping validation for now');
    
  } catch (error) {
    console.error('‚ùå Database validation failed:', error);
    throw error;
  }
}
